import cloudscraper
import re
import os

# æ£€æŸ¥å¹¶å®‰è£… cloudscraper åº“ï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰
# try:
#     import cloudscraper
# except ImportError:
#     print("cloudscraper åº“æœªå®‰è£…ï¼Œæ­£åœ¨å°è¯•å®‰è£…...")
#     os.system('pip install cloudscraper')
#     import cloudscraper
#     print("cloudscraper å®‰è£…å®Œæˆã€‚")

scraper = cloudscraper.create_scraper()  # èƒ½ç»•è¿‡ Cloudflare 5s challenge

# å®šä¹‰æ–‡ä»¶è·¯å¾„
OUTPUT_FILENAME = "download_urls.txt"

def extract_post_ids(html: str) -> list[int]:
    """ä»åˆ—è¡¨é¡µHTMLä¸­æå–æ‰€æœ‰å¸–å­çš„IDã€‚"""
    # åŒ¹é… href=["./]?posts/(\d+)
    pattern = r'href=["\'](?:\.?/)?posts/(\d+)'
    return list(map(int, re.findall(pattern, html)))

def extract_download_url(html: str) -> str | None:
    """ä»å¸–å­è¯¦æƒ…é¡µHTMLä¸­æå–åŸå›¾ä¸‹è½½é“¾æ¥ã€‚"""
    # åŒ¹é…ä»¥ https://api.anime-pictures.net/pictures/download_image/ å¼€å¤´çš„é“¾æ¥
    pattern = r'https://api\.anime-pictures\.net/pictures/download_image/[^\"]+'
    match = re.search(pattern, html)
    return match.group(0) if match else None

def get_download_url_for_page(page_url: str) -> list[str]:
    """
    è·å–å•ä¸ªåˆ—è¡¨é¡µä¸­æ‰€æœ‰å›¾ç‰‡çš„ä¸‹è½½é“¾æ¥ã€‚
    
    Args:
        page_url: åˆ—è¡¨é¡µçš„URLã€‚
        
    Returns:
        ä¸€ä¸ªåŒ…å«æ‰€æœ‰ä¸‹è½½é“¾æ¥çš„åˆ—è¡¨ã€‚
    """
    print(f"\n--- æ­£åœ¨å¤„ç†åˆ—è¡¨é¡µ: {page_url} ---")
    
    # 1. è®¿é—®åˆ—è¡¨é¡µ
    try:
        resp = scraper.get(page_url)
        print(f"åˆ—è¡¨é¡µçŠ¶æ€ç : {resp.status_code}")
        if resp.status_code != 200:
            print(f"è®¿é—®åˆ—è¡¨é¡µå¤±è´¥ï¼Œè·³è¿‡ã€‚")
            return []
    except Exception as e:
        print(f"è®¿é—®åˆ—è¡¨é¡µæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return []

    # 2. æå–å¸–å­ID
    ids = extract_post_ids(resp.text)
    ids = ids[:80] # æ ¹æ®åŸä»£ç é€»è¾‘ï¼Œè¿™é‡Œå¯ä»¥é™åˆ¶æ•°é‡ï¼Œä½†å¦‚æœæƒ³è·å–å…¨éƒ¨ï¼Œå¯ä»¥å»æ‰æˆ–è°ƒæ•´
    print(f"æå–åˆ° {len(ids)} ä¸ªå¸–å­IDã€‚")
    
    final_urls = []
    
    # 3. éå†å¸–å­IDï¼Œè®¿é—®è¯¦æƒ…é¡µå¹¶æå–ä¸‹è½½é“¾æ¥
    for i, id in enumerate(ids):
        # æ„é€ è¯¦æƒ…é¡µURL
        pic_url = f"https://anime-pictures.net/posts/{id}?by_tag=21508&lang=zh-cn"
        
        try:
            resp_pic = scraper.get(pic_url)
            # print(f"  - å¸–å­ {id} çŠ¶æ€ç : {resp_pic.status_code}")
            
            if resp_pic.status_code == 200:
                download_url = extract_download_url(resp_pic.text)
                if download_url:
                    final_urls.append(download_url)
                else:
                    # print(f"  - å¸–å­ {id} æœªæ‰¾åˆ°ä¸‹è½½é“¾æ¥ã€‚")
                    pass
            
        except Exception as e:
            print(f"è®¿é—®å¸–å­ {id} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            
        # æ‰“å°è¿›åº¦
        if (i + 1) % 10 == 0 or (i + 1) == len(ids):
            print(f"  -> å·²å¤„ç† {i + 1}/{len(ids)} ä¸ªå¸–å­ï¼Œå·²æ”¶é›† {len(final_urls)} ä¸ªé“¾æ¥ã€‚")

    return final_urls

def run_scraper_and_save(start_page: int, end_page: int, base_url_template: str):
    """
    å¾ªç¯éå†æŒ‡å®šé¡µç èŒƒå›´ï¼Œè·å–æ‰€æœ‰ä¸‹è½½é“¾æ¥å¹¶ä¿å­˜åˆ°æ–‡ä»¶ã€‚
    """
    all_download_urls = []
    
    for page in range(start_page, end_page + 1):
        # æ„é€ å½“å‰é¡µçš„URL
        current_url = base_url_template.format(page=page)
        
        # è·å–å½“å‰é¡µçš„æ‰€æœ‰ä¸‹è½½é“¾æ¥
        urls_for_page = get_download_url_for_page(current_url)
        
        # å°†ç»“æœæ·»åŠ åˆ°æ€»åˆ—è¡¨ä¸­
        all_download_urls.extend(urls_for_page)
        
    print(f"\n==========================================")
    print(f"âœ… æ‰€æœ‰é¡µé¢å¤„ç†å®Œæˆã€‚")
    print(f"æ€»å…±æ”¶é›†åˆ° {len(all_download_urls)} ä¸ªä¸‹è½½é“¾æ¥ã€‚")
    
    # å°†æ‰€æœ‰é“¾æ¥å†™å…¥æ–‡ä»¶
    with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:
        # ä½¿ç”¨ set å»é‡åå†å†™å…¥
        unique_urls = set(all_download_urls)
        for url in unique_urls:
            if url: # ç¡®ä¿é“¾æ¥éç©º
                f.write(url + '\n')

    print(f"ğŸ”— æ‰€æœ‰å”¯ä¸€çš„ä¸‹è½½é“¾æ¥å·²ä¿å­˜åˆ°æ–‡ä»¶: **{OUTPUT_FILENAME}**")
    print(f"æ–‡ä»¶ä¸­å…±æœ‰ {len(unique_urls)} ä¸ªå”¯ä¸€çš„é“¾æ¥ã€‚")


# --- è¿è¡Œä¸»ç¨‹åº ---
# åˆ—è¡¨é¡µçš„åŸºç¡€URLæ¨¡æ¿ï¼Œ{page} ä¼šè¢«æ›¿æ¢
BASE_URL_TEMPLATE = "https://anime-pictures.net/posts?page={page}&search_tag=girl&order_by=rating&ldate=4&lang=zh-cn"

# è¿è¡Œå¾ªç¯ä» page 12 åˆ° 20 (åŒ…å« 20)
run_scraper_and_save(start_page=21, end_page=50, base_url_template=BASE_URL_TEMPLATE)